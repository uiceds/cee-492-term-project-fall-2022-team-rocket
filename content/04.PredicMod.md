## Predictive Modeling {.page_break_before}

<div style="text-align: justify">Based on the exploratory data analysis it can be concluded that the differentiation of the classes is challenging. The analytical approach to determining key features for distinguishing different classes is not conclusive and overall, the data delineation is not observed. Hence, four different types of models including the Convolutional Neural Network (CNN) were used to classify the delamination into three categories. The dataset was divided into training and testing data. </div> 

<div style="text-align: justify">The four models used in the preliminary predictive modeling are ‘Decision Tree’, ‘Random Forest’, ‘Regression’, and ‘1-D Convolutional Neural Network’. The explanations of each of these models and the corresponding results are discussed below.  </div>

### 1. Decision Tree
<div style="text-align: justify">The first predictive model applied was the ‘Decision tree’ model, which is a non-parametric supervised learning method that can be used for classification or regression. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules, and the fitter the model. </div> 

<div style="text-align: justify">Decision tree in machine learning has the advantage of being simple to understand, interpret and visualize. The model can be visually represented by a flowchart of decisions. It can also implicitly perform variable screening or feature selection to determine the most dominant variables that can govern the classification more than the others. However, it also comes with some limitations. For instance, decision tree learners can create over-complex trees that do not generalize the data well, causing data overfitting. In addition, the predictions of decision trees are neither smooth nor continuous, so they do not perform well with extrapolation. </div> 

<div style="text-align: justify">The decision tree classifier was created using 3 labels -one for each class- and 12 variables. These variables are the 10 first PCA modes of the data, the dominant FFT frequency, and its corresponding amplitude. The first 10 PCA modes were selected as they cover more than 90 % of the variance. Then, leaves having more than 90% combined purity were merged before applying the model to avoid overfitting. About 60 % of the data from each class was used for training the model. Then, the model was tested on the complete dataset.  </div> 

<div style="text-align: justify">The summary of the results is shown in Table 3. The accuracy achieved on training data for classes 1, 2, and 3 were around 86%, 83%, and 46%, respectively. This resulted in an overall accuracy of about 82% on the training data. Due to the lower number of Class 3 data points, the model prediction accuracy is lower for class 3 than for the other two classes. The confusion matrices achieved by using this model are shown in Figure 12 and Figure 13. When applied to the complete dataset, the achieved accuracy was around 67%, which means that the accuracy of the unseen 40% of data was about 45%. </div> 

![
**Confusion matrix of the training data for the ‘Decision Tree’ model**
](https://user-images.githubusercontent.com/112973532/202886315-d628336a-e1a5-4879-831e-92af119b0257.png "Tall image"){#fig:tall-image height=2in}

![
**Confusion matrix of full dataset for the ‘Decision Tree’ model**
](https://user-images.githubusercontent.com/112973532/202886343-5f78d819-735d-4796-a8dc-158e00ab9dfe.png "Tall image"){#fig:tall-image height=2in}

### 2. Random Forest
<div style="text-align: justify">To avoid the problem of overfitting, we have also tried to apply the ‘Random Forest’ classifier. A random forest is a meta-estimator that fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The parameters used for building our model are listed below: </div>

•	Number of random features = 6

•	Trees = 10

•	Portions of samples per tree = 0.6

•	Maximum tree depth = 17

<div style="text-align: justify">The accuracy obtained by the model using the numbers mentioned above was around 64 %. Values of these parameters were changed to default values (default of the package: DecisionTree.jl), and then the new model was generated and tested. With the default values, the achieved accuracy was around 66 %. The confusion matrix obtained from this model is shown in Figure 14. </div> 

![
**Confusion matrix of full dataset for the ‘Random Forest’ model**
](https://user-images.githubusercontent.com/112973532/202886385-56bd9337-ff2b-44c6-afaf-c12196b0e5ec.png "Tall image"){#fig:tall-image height=2in}

### 3. Regression
<div style="text-align: justify">Regression is a method for creating a model of the relationship between one or more independent variables and one or more dependent variables, where variation in the dependent variables is used to explain variation in the dependent variables. The regression method prepared for this project considered a total of 260 independent variables and 1 dependent variables (classification). The regression model was optimized by minimizing a maximum square error function, used a learning rate of 0.001 and 10,000 steps. 60% of the entire data set was used as training for the regression model.</div>

<div style="text-align: justify">Ideally, regression model results should match actual observations. As observed in Figure 15, similar values were predicted for the three observed classes, which unables this system alone to accurately classify the predictions. This finding confirms that regresion model is not the most adequate tool for a classification model.</div>

![
**Class Predictions vs Actual Classes, from regresion-based machine learning model**
](https://user-images.githubusercontent.com/112973477/202911859-f923154f-6e92-4d16-8eaf-0d5687c13693.png "Tall image"){#fig:tall-image height=2in}


### 4. Convolutional Neural Network
<div style="text-align: justify">The Convolutional Neural Network (CNN) was created for the GPR data at different locations. Out of the approximately 16000 signals, around 9800 random signals were used to train the model. Each signal has 260 data points. The 1D CNN is composed of several layers. The first layer was a convolution layer with a filter size = 5 with 1 input channel to 8 output channels of training data. The padding was of size= 2 and Relu was the activation function. The next layer was a Maxpool layer of size= 4. The third layer is another convolutional layer with a filter of a size = 5 and padding of size= 2 with 8 channels to 3 channels. The fourth layer with a Maxpool layer of size =5 and the fifth layer is a dense layer of size (39,3) based on the input size and the number of channels. Finally, a Softmax layer is applied to discriminate between different classes. The batch size was 10 and learning rate was 0.001.</div>

<div style="text-align: justify">Figure 16 show some diagnostics of our model training, include loss and accuracy curves as a function of gradient descent step and a confusion matrix of the final results. The accuracy of 52% is achieved based on the convolutional neural network. As observed, after certain epochs the accuracy becomes constant and does not improve. The model was further executed for different channels as well as different activation functions like Relu, Tanh, and its combination. However, the output was close to 52% accuracy with no improvement. Further modification and improvements in the CNN model are required.</div>

![
**Performance of the CNN model. It shows the loss and accuracy curves as a function of gradient descent step and a confusion matrix**
](https://user-images.githubusercontent.com/112973532/202886385-56bd9337-ff2b-44c6-afaf-c12196b0e5ec.png "Tall image"){#fig:tall-image height=2in}


### Discussions
<div style="text-align: justify">As mentioned, the data is classified into three (3) different labels/classes. However, the amount of data samples for the three classes is not the same. For example, class 3 represents only about 7.5% of the entire dataset, while class 1 is approximately 50%. 
Table 3 provides a accuracy/performance based summary of the effectiveness of each of the studied predicitve models.The accuracy of predictive models is linked to the sample size of each class. An imbalanced dataset may cause predictive models to be biased towards learning more about the dominant classes causing higher accuracy of prediction of these classes. Moreover further improvement in CNN model by optimizing the model parameters and layers is required. While Decision  Regresion model performance was measured in terms of MSE, which may not be comparable to other accuracy metrics. Tree and Random Forest models provided relatively high accuracy levels on training data (above 75%), 1-D CNN model provided the highest accuracy on unseen data with 52%.</div>


<div style="text-align: center">**Table 3. Summary of results of predictive modeling**</div>
| Models | Training data | Accuracy On Training data | Accuracy On Full dataset | Accuracy On Unseen data |
|:-------------|:-------------:|:-------------:|:-------------:|:-------------:|
| Decision Tree | 60 % | 82 % | 67 % | 45 % |
| Random Forest | 60 % | 76 % | 65 % | 49 % |
| Regression | 60 % | 6.1 (MSE) | 6.2 (MSE) | 5.5 (MSE) |
| 1-D CNN |	60% | 52 % | 51 % | 52 % |		